---
layout: home
title: Chinmay Savadikar
redirect_from:
    - /aboutme
    - /aboutme/
---	

<div class="row content-container header">
	<div class="col popout-container rounded decorate-purple">
			<p class="card-subtitle mt-2"><b>Chinmay Savadikar</b></p>
			<p class="card-subtitle mt-1">Ph.D. Student, <a href="https://ece.ncsu.edu/">Department of ECE</a></p>
			<p class="card-subtitle mt-1">North Carolina State University</p>
			<p class="card-subtitle info-card-email mt-1">csavadi [at] ncsu [dot] edu</p>
			<ul class="list-inline links mt-1">
				<li class="list-inline-item"><a href="#about">About</a></li>
				<li class="list-inline-item"><a href="#publications">Publications</a></li>
				<li class="list-inline-item"><a href="/cv.pdf">CV</a></li>
			</ul>
	</div>
</div>

<div class="title bg-white" id="about">
	<h1>About</h1>
</div>

<div class="row content-container border-bottom">

	<!-- <div class="col-sm-8 col-sm-pull8 col-md-8 col-md-pull8 bio" align="justify"> -->
	<div class="col-xs-8 col-md-8 bio" align="justify">
	
		<p>I am a first year Ph.D. student at North Carolina State University advised by <a href="https://ece.ncsu.edu/people/twu19/">Dr. Tianfu Wu</a>. My research interest lies in Continual Learning, with broader interest in efficient and robust learning for visual tasks.</p>

		<p>Prior to starting my Ph.D., I worked with the <a href="https://precisionsustainableag.org/">Precision Sustainable Agriculture</a> initiative at NC State to build Computer Vision and Software solutions for problems in agriculture.</p>

		<p>I worked as a Machine Learning Engineer at Persistent Systems before coming back to academia. At Persistent, I worked on Deep Learning for Medical Imaging and large scale document recognition. I also spent some time developing internal SDKs for the Data Science team, and setting up MLOps frameworks.</p>
		
	</div>

	<!-- <div class="col-sm-4 col-sm-push-4 col-md-4 col-md-push-4"> -->
	<div class="col-md-4">
		<div class="card rounded decorate-blue margin-right-zero">

			<div class="news-title"><h3>News</h3></div>
			<div class="news" align="justify">
				<span class="badge badge-secondary">Dec 2023</span><p>New preprint - <b>GIFT: Generative Interpretable Fine-Tuning Transformers</b> available on ArXiv!
					<br><b>Link</b>: <a href="https://arxiv.org/abs/2312.00700">https://arxiv.org/abs/2312.00700</a>.</p>
			</div>
			<div class="news" align="justify">
				<span class="badge badge-secondary">Mar 2023</span><p>New preprint - <b>Transforming Transformers for Resilient Lifelong Learning</b> available on ArXiv!
					<br><b>Link</b>: <a href="https://arxiv.org/abs/2303.08250">https://arxiv.org/abs/2303.08250</a>.</p>
			</div>
			<div class="news" align="justify">
				<span class="badge badge-secondary">Aug 2022</span><p>Started as a Ph.D. student as a part of <a href="https://research.ece.ncsu.edu/ivmcl/">Laboratory for interpretable Visual Modeling, Computing and Learning (iVMCL)</a>.</p>
			</div>
		</div>
		
	</div>
	
</div>

<div class="title sticky-top bg-white" id="publications">
	<h1>Publications</h1>
</div>

<div class="row content-container border-bottom">
	<div class="row popout-container rounded justify-content-center decorate-yellow align-items-center margin-bottom">
		<div class="col-xs-4 col-md-4">
			<img class="pub-thumbnail mt-1 mb-1" src="assets/images/gift.jpg">
		</div>
		<div class="col-md-8 align-items-center">
			<a href="https://arxiv.org/abs/2312.00700" class="links">GIFT: Generative Interpretable Fine-Tuning Transformers</a>
			<br/><venue>ArXiv Preprint</venue>
			<br>
			<b>Chinmay Savadikar</b>,
			Xi Song,
			<a href="https://ece.ncsu.edu/people/twu19/">Tianfu Wu</a>
			<br>
			<a href="https://arxiv.org/pdf/2312.00700.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>
			<a href="https://savadikarc.github.io/gift/" class="badge badge-secondary badge-sm text-decoration-none mb-1">web</a>
			<a href="https://github.com/savadikarc/gift" class="badge badge-secondary badge-sm text-decoration-none mb-1">code</a>

			<p>
				We present GIFT (Generative Interpretable Fine-tuning Transformers) for fine-tuning pretrained (often large) Transformer models at downstream tasks in a 
				parameter-efficient way with built-in interpretability. Our GIFT is a hyper-Transformer which take as input the pretrained parameters of the projection 
				layer to generate its fine-tuning parameters using a proposed Parameter-to-Cluster Attention (PaCa). The PaCa results in a simple clustering-based forward 
				explainer that plays the role of semantic segmentation in testing.
			</p>
		</div>
	</div>
	<div class="row popout-container rounded justify-content-center decorate-yellow align-items-center">
		<div class="col-xs-4 col-md-4">
			<img class="pub-thumbnail mt-1 mb-1" src="assets/images/artihippo.jpg">
		</div>
		<div class="col-md-8 align-items-center">
			<a href="https://arxiv.org/abs/2303.08250" class="links">Transforming Transformers for Resilient Lifelong Learning</a>
			<br/><venue>ArXiv Preprint</venue>
			<br>
			<b>Chinmay Savadikar</b>,
			Michelle Dai,
			<a href="https://ece.ncsu.edu/people/twu19/">Tianfu Wu</a>
			<br>
			<a href="https://arxiv.org/pdf/2303.08250.pdf" class="badge badge-secondary badge-sm text-decoration-none mb-1">pdf</a>

			<p>
				We present a method to add lightweight, learnable parameters to Vision Transformers while leveraging parameter-heavy, but stable components.
				We show that the final linear projection layer in the multi-head self-attention (MHSA) block can be used as this light-weight module using a Mixture of Experts framework.
				While most of the prior methods which address this problem induce learnable parameters at every layer, or heuristically choose where to do so, we use Neural Architecture Search to determine this automatically.
				We use SPOS Neural Architecture Search and propose a task-similarity oriented sampling strategy to replace the uniform sampling and achieve better performance and efficiency than uniform sampling.
				We test the proposed method on the challenging Visual Domain Decathlon benchmark and obtain strong empirical performance, and show that our proposed method is complementary to prompt based approaches.
			</p>
		</div>
	</div>
</div>
